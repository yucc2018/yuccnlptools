2020-07-12 18:27:15,961 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 18:27:15,972 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 18:27:15,973 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 18:28:11,211 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 18:28:11,220 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 18:28:11,221 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 18:28:11,228 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 18:28:11,230 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 18:28:11,234 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 18:28:11,332 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 18:28:11,333 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 18:28:11,334 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 18:28:11,335 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 18:28:11,336 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 18:28:11,336 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 18:28:11,337 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 18:28:11,338 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 18:28:11,338 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 18:28:11,339 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 18:28:11,339 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 18:28:11,387 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 18:28:14,399 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 18:28:14,401 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 18:34:03,999 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 18:34:04,006 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 18:34:04,007 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 18:34:04,013 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 18:34:04,014 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 18:34:04,018 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 18:34:04,144 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 18:34:04,145 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 18:34:04,147 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 18:34:04,148 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 18:34:04,149 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 18:34:04,150 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 18:34:04,151 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 18:34:04,151 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 18:34:04,152 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 18:34:04,152 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 18:34:04,153 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 18:34:04,203 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 18:34:07,196 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 18:34:07,198 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 19:39:21,047 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 19:39:21,056 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 19:39:21,057 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 19:39:21,064 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:39:21,192 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:39:21,196 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:39:21,198 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:39:21,198 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 19:39:21,199 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 19:39:21,200 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 19:39:21,202 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 19:39:21,203 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 19:39:21,204 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 19:39:21,205 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:39:21,205 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:39:21,206 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:39:21,206 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:39:21,258 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 19:39:24,269 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 19:39:24,272 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 19:39:24,277 genernal.py:  186             __init__() INFO     topic: 体育
2020-07-12 19:39:58,399 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 19:39:58,408 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 19:39:58,409 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 19:39:58,415 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:39:58,417 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:39:58,421 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:39:58,536 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:39:58,537 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 19:39:58,539 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 19:39:58,540 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 19:39:58,541 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 19:39:58,541 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 19:39:58,542 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 19:39:58,543 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:39:58,544 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:39:58,544 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:39:58,545 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:39:58,591 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 19:40:01,694 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 19:40:01,696 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 19:40:01,702 genernal.py:  187             __init__() INFO     topic: 体育
2020-07-12 19:40:02,740 genernal.py:  187             __init__() INFO     topic: 数码产品
2020-07-12 19:40:04,656 genernal.py:  187             __init__() INFO     topic: 美食
2020-07-12 19:40:08,677 genernal.py:  187             __init__() INFO     topic: 音乐
2020-07-12 19:40:11,020 genernal.py:  187             __init__() INFO     topic: 电影
2020-07-12 19:41:29,159 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 19:41:29,168 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 19:41:29,169 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 19:41:29,175 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:41:29,176 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:41:29,181 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:41:29,303 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:41:29,304 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 19:41:29,305 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 19:41:29,305 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 19:41:29,306 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 19:41:29,307 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 19:41:29,308 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 19:41:29,308 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:41:29,309 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:41:29,309 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:41:29,310 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:41:29,352 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 19:41:32,455 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 19:41:32,457 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 19:41:32,462 genernal.py:  188             __init__() INFO     topic: 体育
2020-07-12 19:41:33,407 genernal.py:  188             __init__() INFO     topic: 数码产品
2020-07-12 19:41:35,069 genernal.py:  188             __init__() INFO     topic: 美食
2020-07-12 19:41:38,717 genernal.py:  188             __init__() INFO     topic: 音乐
2020-07-12 19:41:40,902 genernal.py:  188             __init__() INFO     topic: 电影
2020-07-12 19:42:35,087 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 19:42:35,096 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 19:42:35,097 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 19:42:35,105 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:42:35,107 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:42:35,112 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:42:35,233 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:42:35,234 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 19:42:35,236 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 19:42:35,237 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 19:42:35,238 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 19:42:35,239 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 19:42:35,240 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 19:42:35,240 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:42:35,241 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:42:35,241 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:42:35,242 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:42:35,295 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 19:42:38,371 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 19:42:38,372 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 19:42:38,378 genernal.py:  188             __init__() INFO     topic: 体育
2020-07-12 19:42:39,425 genernal.py:  188             __init__() INFO     topic: 数码产品
2020-07-12 19:42:41,209 genernal.py:  188             __init__() INFO     topic: 美食
2020-07-12 19:42:45,661 genernal.py:  188             __init__() INFO     topic: 音乐
2020-07-12 19:42:48,243 genernal.py:  188             __init__() INFO     topic: 电影
2020-07-12 19:42:51,660 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 19:43:10,491 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 19:43:10,499 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 19:43:10,500 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 19:43:10,507 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:43:10,509 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:43:10,514 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:43:10,631 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:43:10,632 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 19:43:10,634 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 19:43:10,635 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 19:43:10,636 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 19:43:10,637 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 19:43:10,638 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 19:43:10,638 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:43:10,639 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:43:10,639 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:43:10,640 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:43:10,688 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 19:43:13,767 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 19:43:13,769 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 19:43:13,774 genernal.py:  188             __init__() INFO     topic: 体育
2020-07-12 19:43:14,796 genernal.py:  188             __init__() INFO     topic: 数码产品
2020-07-12 19:43:16,648 genernal.py:  188             __init__() INFO     topic: 美食
2020-07-12 19:43:20,713 genernal.py:  188             __init__() INFO     topic: 音乐
2020-07-12 19:43:23,039 genernal.py:  188             __init__() INFO     topic: 电影
2020-07-12 19:43:27,017 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 19:44:25,200 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 19:44:25,209 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 19:44:25,210 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 19:44:25,217 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:44:25,219 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:44:25,224 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 19:44:25,341 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 19:44:25,342 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 19:44:25,343 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 19:44:25,344 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 19:44:25,345 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 19:44:25,346 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 19:44:25,347 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 19:44:25,347 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:44:25,348 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:44:25,348 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:44:25,349 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 19:44:25,400 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 19:44:28,354 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 19:44:28,355 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 19:44:28,360 genernal.py:  188             __init__() INFO     topic: 体育
2020-07-12 19:44:29,399 genernal.py:  188             __init__() INFO     topic: 数码产品
2020-07-12 19:44:31,182 genernal.py:  188             __init__() INFO     topic: 美食
2020-07-12 19:44:35,249 genernal.py:  188             __init__() INFO     topic: 音乐
2020-07-12 19:44:37,507 genernal.py:  188             __init__() INFO     topic: 电影
2020-07-12 19:44:41,483 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 19:44:42,054 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 19:44:42,124 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139916478274192 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 19:44:42,152 filelock.py:  274              acquire() INFO     Lock 139916478274192 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 19:44:42,153 genernal.py:  128             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 19:44:42,153 filelock.py:  315              release() DEBUG    Attempting to release lock 139916478274192 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 19:44:42,154 filelock.py:  318              release() INFO     Lock 139916478274192 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:11:01,216 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:11:01,225 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:11:01,227 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:11:01,235 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:11:01,236 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:11:01,241 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:11:01,367 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:11:01,368 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:11:01,371 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:11:01,372 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:11:01,373 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:11:01,374 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:11:01,375 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:11:01,376 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:11:01,376 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:11:01,377 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:11:01,377 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:11:01,428 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:11:04,378 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:11:04,379 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:11:04,382 genernal.py:  188             __init__() INFO     topic: 体育
2020-07-12 20:11:05,452 genernal.py:  188             __init__() INFO     topic: 数码产品
2020-07-12 20:11:07,261 genernal.py:  188             __init__() INFO     topic: 美食
2020-07-12 20:11:11,244 genernal.py:  188             __init__() INFO     topic: 音乐
2020-07-12 20:11:13,555 genernal.py:  188             __init__() INFO     topic: 电影
2020-07-12 20:11:17,001 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:11:17,548 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:11:17,621 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139688934867464 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:11:17,623 filelock.py:  274              acquire() INFO     Lock 139688934867464 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:11:17,625 genernal.py:  128             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:11:17,626 filelock.py:  315              release() DEBUG    Attempting to release lock 139688934867464 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:11:17,627 filelock.py:  318              release() INFO     Lock 139688934867464 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:12:31,508 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:12:31,515 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:12:31,516 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:12:31,522 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:12:31,631 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:12:31,635 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:12:31,636 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:12:31,637 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:12:31,638 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:12:31,639 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:12:31,640 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:12:31,641 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:12:31,642 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:12:31,643 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:12:31,643 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:12:31,644 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:12:31,644 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:12:31,694 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:12:34,830 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:12:34,831 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:12:34,836 genernal.py:  188             __init__() INFO     topic: 体育
2020-07-12 20:12:35,886 genernal.py:  188             __init__() INFO     topic: 数码产品
2020-07-12 20:12:37,625 genernal.py:  188             __init__() INFO     topic: 美食
2020-07-12 20:12:41,650 genernal.py:  188             __init__() INFO     topic: 音乐
2020-07-12 20:12:43,839 genernal.py:  188             __init__() INFO     topic: 电影
2020-07-12 20:12:47,243 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:12:47,791 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:12:47,886 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140138272387592 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:12:47,889 filelock.py:  274              acquire() INFO     Lock 140138272387592 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:12:47,890 genernal.py:  128             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:12:47,890 filelock.py:  315              release() DEBUG    Attempting to release lock 140138272387592 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:12:47,891 filelock.py:  318              release() INFO     Lock 140138272387592 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:15:16,889 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:15:16,896 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:15:16,897 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:15:16,903 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:15:17,039 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:15:17,043 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:15:17,044 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:15:17,045 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:15:17,046 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:15:17,047 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:15:17,047 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:15:17,048 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:15:17,049 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:15:17,049 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:15:17,050 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:15:17,051 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:15:17,051 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:15:17,105 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:15:20,073 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:15:20,075 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:15:20,080 genernal.py:  188             __init__() INFO     topic: 体育
2020-07-12 20:15:21,141 genernal.py:  188             __init__() INFO     topic: 数码产品
2020-07-12 20:15:22,969 genernal.py:  188             __init__() INFO     topic: 美食
2020-07-12 20:15:27,022 genernal.py:  188             __init__() INFO     topic: 音乐
2020-07-12 20:15:29,379 genernal.py:  188             __init__() INFO     topic: 电影
2020-07-12 20:15:34,582 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:15:35,172 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:15:35,221 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140310508179920 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:15:35,223 filelock.py:  274              acquire() INFO     Lock 140310508179920 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:15:35,224 genernal.py:  119             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:15:35,226 filelock.py:  315              release() DEBUG    Attempting to release lock 140310508179920 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:15:35,228 filelock.py:  318              release() INFO     Lock 140310508179920 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:17:31,551 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:17:31,559 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:17:31,560 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:17:31,566 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:17:31,567 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:17:31,571 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:17:31,684 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:17:31,685 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:17:31,687 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:17:31,688 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:17:31,689 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:17:31,690 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:17:31,691 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:17:31,692 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:17:31,692 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:17:31,693 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:17:31,693 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:17:31,745 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:17:34,832 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:17:34,834 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:17:34,840 genernal.py:  188             __init__() INFO     topic: 体育
2020-07-12 20:17:36,475 genernal.py:  188             __init__() INFO     topic: 数码产品
2020-07-12 20:17:38,073 genernal.py:  188             __init__() INFO     topic: 美食
2020-07-12 20:17:41,714 genernal.py:  188             __init__() INFO     topic: 音乐
2020-07-12 20:17:43,768 genernal.py:  188             __init__() INFO     topic: 电影
2020-07-12 20:17:47,146 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:17:47,647 genernal.py:  144 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:17:47,693 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139631903951504 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:17:47,695 filelock.py:  274              acquire() INFO     Lock 139631903951504 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:17:47,697 genernal.py:  119             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:17:47,698 filelock.py:  315              release() DEBUG    Attempting to release lock 139631903951504 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:17:47,699 filelock.py:  318              release() INFO     Lock 139631903951504 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:25:32,099 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:25:32,108 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:25:32,109 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:25:32,117 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:25:32,249 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:25:32,253 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:25:32,255 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:25:32,256 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:25:32,257 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:25:32,258 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:25:32,258 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:25:32,259 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:25:32,260 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:25:32,261 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:25:32,261 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:25:32,262 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:25:32,262 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:25:32,309 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:25:35,233 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:25:35,235 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:25:35,242 genernal.py:  189             __init__() INFO     topic: 体育
2020-07-12 20:25:36,261 genernal.py:  189             __init__() INFO     topic: 数码产品
2020-07-12 20:25:38,063 genernal.py:  189             __init__() INFO     topic: 美食
2020-07-12 20:25:41,988 genernal.py:  189             __init__() INFO     topic: 音乐
2020-07-12 20:25:44,281 genernal.py:  189             __init__() INFO     topic: 电影
2020-07-12 20:25:47,603 genernal.py:  145 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:25:48,135 genernal.py:  145 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:25:48,199 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139953248928208 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:25:48,201 filelock.py:  274              acquire() INFO     Lock 139953248928208 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:25:48,202 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:25:48,203 filelock.py:  315              release() DEBUG    Attempting to release lock 139953248928208 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:25:48,204 filelock.py:  318              release() INFO     Lock 139953248928208 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:31:45,568 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:31:45,577 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:31:45,578 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:31:45,585 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:31:45,586 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:31:45,590 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:31:45,708 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:31:45,708 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:31:45,710 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:31:45,711 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:31:45,711 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:31:45,712 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:31:45,713 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:31:45,714 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:31:45,714 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:31:45,715 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:31:45,715 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:31:45,767 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:31:48,773 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:31:48,775 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:31:48,781 genernal.py:  190             __init__() INFO     topic: 体育
2020-07-12 20:31:49,833 genernal.py:  190             __init__() INFO     topic: 数码产品
2020-07-12 20:31:51,721 genernal.py:  190             __init__() INFO     topic: 美食
2020-07-12 20:31:56,349 genernal.py:  190             __init__() INFO     topic: 音乐
2020-07-12 20:31:58,640 genernal.py:  190             __init__() INFO     topic: 电影
2020-07-12 20:32:02,042 genernal.py:  146 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:32:02,616 genernal.py:  146 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:32:02,673 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140348162087568 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:32:02,675 filelock.py:  274              acquire() INFO     Lock 140348162087568 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:32:02,676 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:32:02,677 filelock.py:  315              release() DEBUG    Attempting to release lock 140348162087568 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:32:02,678 filelock.py:  318              release() INFO     Lock 140348162087568 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:36:49,154 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:36:49,161 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:36:49,162 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:36:49,169 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:36:49,278 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:36:49,282 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:36:49,283 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:36:49,284 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:36:49,285 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:36:49,285 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:36:49,286 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:36:49,286 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:36:49,287 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:36:49,288 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:36:49,288 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:36:49,288 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:36:49,289 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:36:49,333 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:36:52,339 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:36:52,340 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:36:52,344 genernal.py:  190             __init__() INFO     topic: 体育
2020-07-12 20:36:53,387 genernal.py:  190             __init__() INFO     topic: 数码产品
2020-07-12 20:36:55,299 genernal.py:  190             __init__() INFO     topic: 美食
2020-07-12 20:36:59,316 genernal.py:  190             __init__() INFO     topic: 音乐
2020-07-12 20:37:01,609 genernal.py:  190             __init__() INFO     topic: 电影
2020-07-12 20:37:04,995 genernal.py:  146 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:37:05,504 genernal.py:  146 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:37:05,552 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140105079089808 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:37:05,554 filelock.py:  274              acquire() INFO     Lock 140105079089808 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:37:05,556 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:37:05,557 filelock.py:  315              release() DEBUG    Attempting to release lock 140105079089808 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:37:05,558 filelock.py:  318              release() INFO     Lock 140105079089808 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:39:08,633 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:39:08,640 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:39:08,641 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:39:08,648 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:39:08,650 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:39:08,655 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:39:08,764 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:39:08,765 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:39:08,767 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:39:08,768 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:39:08,768 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:39:08,769 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:39:08,770 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:39:08,771 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:39:08,771 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:39:08,772 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:39:08,772 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:39:08,824 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:39:11,884 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:39:11,885 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:39:11,890 genernal.py:  191             __init__() INFO     topic: 体育
2020-07-12 20:39:12,930 genernal.py:  191             __init__() INFO     topic: 数码产品
2020-07-12 20:39:14,738 genernal.py:  191             __init__() INFO     topic: 美食
2020-07-12 20:39:18,727 genernal.py:  191             __init__() INFO     topic: 音乐
2020-07-12 20:39:21,001 genernal.py:  191             __init__() INFO     topic: 电影
2020-07-12 20:39:24,428 genernal.py:  147 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:39:24,942 genernal.py:  147 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:39:24,989 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140347517192848 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:39:24,991 filelock.py:  274              acquire() INFO     Lock 140347517192848 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:39:24,993 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:39:38,271 filelock.py:  315              release() DEBUG    Attempting to release lock 140347517192848 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:39:38,273 filelock.py:  318              release() INFO     Lock 140347517192848 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:44:30,675 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:44:30,686 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:44:30,687 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:44:30,694 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:44:30,696 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:44:30,700 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:44:30,816 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:44:30,817 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:44:30,818 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:44:30,819 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:44:30,820 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:44:30,821 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:44:30,822 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:44:30,822 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:44:30,823 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:44:30,823 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:44:30,824 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:44:30,869 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:44:34,032 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:44:34,034 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:44:34,038 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 20:44:34,965 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 20:44:37,547 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 20:44:41,135 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 20:44:43,267 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 20:44:46,643 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:44:47,186 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:44:47,244 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140416217760208 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:44:47,247 filelock.py:  274              acquire() INFO     Lock 140416217760208 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:44:47,249 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:44:47,249 filelock.py:  315              release() DEBUG    Attempting to release lock 140416217760208 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:44:47,250 filelock.py:  318              release() INFO     Lock 140416217760208 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:46:22,750 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:46:22,759 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:46:22,760 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:46:22,767 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:46:22,878 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:46:22,883 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:46:22,884 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:46:22,885 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:46:22,886 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:46:22,887 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:46:22,888 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:46:22,889 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:46:22,890 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:46:22,891 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:46:22,891 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:46:22,892 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:46:22,892 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:46:22,942 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:46:25,987 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:46:25,988 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:46:26,445 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 20:46:27,488 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 20:46:29,314 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 20:46:33,284 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 20:46:35,577 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 20:46:38,976 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:46:39,524 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:46:39,589 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139925185368584 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:46:39,592 filelock.py:  274              acquire() INFO     Lock 139925185368584 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:46:39,593 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:46:53,433 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:46:53,434 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-0
2020-07-12 20:46:53,436 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1917, 100, 1763, 100, 1989, 100, 100, 100, 100, 1890, 1775, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:46:53,437 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:46:53,437 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-2
2020-07-12 20:46:53,438 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1742, 100, 100, 1812, 100, 1778, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:46:53,439 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:46:53,440 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-3
2020-07-12 20:46:53,440 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 11085, 2050, 1989, 6921, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:46:53,441 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:46:53,442 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-4
2020-07-12 20:46:53,443 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1855, 100, 1916, 100, 100, 100, 100, 1812, 1865, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:46:53,443 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:46:53,444 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-0
2020-07-12 20:46:53,445 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1989, 100, 100, 100, 1989, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:47:28,552 genernal.py:  170             __init__() INFO     Saving features into cached file /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank [took 35.100 s]
2020-07-12 20:47:28,553 filelock.py:  315              release() DEBUG    Attempting to release lock 139925185368584 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:47:28,555 filelock.py:  318              release() INFO     Lock 139925185368584 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:47:28,558 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 20:47:29,592 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 20:47:32,388 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 20:47:36,970 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 20:47:39,207 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 20:47:42,591 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:47:42,984 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:47:43,036 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139925594069200 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_300_smp-rank.lock
2020-07-12 20:47:43,039 filelock.py:  274              acquire() INFO     Lock 139925594069200 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_300_smp-rank.lock
2020-07-12 20:47:43,040 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:47:43,762 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:47:43,764 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-1
2020-07-12 20:47:43,765 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 1742, 100, 100, 100, 1962, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:47:43,766 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:47:43,767 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-2
2020-07-12 20:47:43,767 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1989, 1018, 1011, 1014, 100, 100, 100, 100, 1989, 100, 100, 100, 100, 100, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:47:43,768 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:47:43,769 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-3
2020-07-12 20:47:43,770 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 2184, 1840, 1986, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:47:43,770 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:47:43,771 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-4
2020-07-12 20:47:43,772 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1855, 100, 100, 100, 100, 100, 100, 1916, 1815, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:47:43,772 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:47:43,773 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-5
2020-07-12 20:47:43,774 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1740, 100, 100, 100, 100, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:47:45,674 genernal.py:  170             __init__() INFO     Saving features into cached file /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_300_smp-rank [took 1.899 s]
2020-07-12 20:47:45,675 filelock.py:  315              release() DEBUG    Attempting to release lock 139925594069200 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_300_smp-rank.lock
2020-07-12 20:47:45,677 filelock.py:  318              release() INFO     Lock 139925594069200 released on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_300_smp-rank.lock
2020-07-12 20:49:48,119 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:49:48,128 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2020-07-12 20:49:48,129 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:49:48,136 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:49:48,137 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:49:48,141 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:49:48,250 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:49:48,251 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:49:48,253 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:49:48,254 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:49:48,254 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:49:48,255 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:49:48,256 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:49:48,257 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:49:48,258 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:49:48,258 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:49:48,258 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:49:48,312 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:49:51,223 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:49:51,225 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:49:51,230 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 20:49:52,249 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 20:49:54,083 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 20:49:57,939 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 20:50:00,232 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 20:50:03,541 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:50:04,101 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:50:04,167 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139801676210696 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:50:04,169 filelock.py:  274              acquire() INFO     Lock 139801676210696 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:50:09,046 genernal.py:  119             __init__() INFO     Loading features from cached file /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank [took 4.875 s]
2020-07-12 20:50:09,048 filelock.py:  315              release() DEBUG    Attempting to release lock 139801676210696 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:50:09,049 filelock.py:  318              release() INFO     Lock 139801676210696 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_300_smp-rank.lock
2020-07-12 20:50:09,052 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 20:50:10,027 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 20:50:11,843 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 20:50:15,793 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 20:50:18,107 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 20:50:21,445 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:50:22,717 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:50:22,789 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139802090363088 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_300_smp-rank.lock
2020-07-12 20:50:22,791 filelock.py:  274              acquire() INFO     Lock 139802090363088 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_300_smp-rank.lock
2020-07-12 20:50:22,978 genernal.py:  119             __init__() INFO     Loading features from cached file /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_300_smp-rank [took 0.186 s]
2020-07-12 20:50:22,980 filelock.py:  315              release() DEBUG    Attempting to release lock 139802090363088 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_300_smp-rank.lock
2020-07-12 20:50:22,981 filelock.py:  318              release() INFO     Lock 139802090363088 released on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_300_smp-rank.lock
2020-07-12 20:50:22,995 trainer.py:  208             __init__() INFO     You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
2020-07-12 20:50:23,026 trainer.py:  440                train() INFO     ***** Running training *****
2020-07-12 20:50:23,027 trainer.py:  441                train() INFO       Num examples = 70206
2020-07-12 20:50:23,030 trainer.py:  442                train() INFO       Num Epochs = 3
2020-07-12 20:50:23,030 trainer.py:  443                train() INFO       Instantaneous batch size per device = 32
2020-07-12 20:50:23,031 trainer.py:  444                train() INFO       Total train batch size (w. parallel, distributed & accumulation) = 32
2020-07-12 20:50:23,031 trainer.py:  445                train() INFO       Gradient Accumulation steps = 1
2020-07-12 20:50:23,032 trainer.py:  446                train() INFO       Total optimization steps = 6582
2020-07-12 20:50:23,033 trainer.py:  468                train() INFO       Starting fine-tuning.
2020-07-12 20:53:26,306 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 20:53:26,336 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False
2020-07-12 20:53:26,338 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 20:53:26,345 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:53:26,348 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:53:26,353 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 20:53:26,355 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 20:53:26,356 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 20:53:26,358 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 20:53:26,359 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 20:53:26,360 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 20:53:26,361 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 20:53:26,363 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 20:53:26,363 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:53:26,364 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:53:26,365 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:53:26,366 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 20:53:26,413 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 20:53:29,851 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 20:53:29,852 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 20:53:29,858 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 20:53:30,908 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 20:53:32,723 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 20:53:36,756 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 20:53:39,077 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 20:53:42,538 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:53:43,198 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:53:43,262 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139670792534616 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:53:43,264 filelock.py:  274              acquire() INFO     Lock 139670792534616 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:53:43,266 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:53:56,916 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:53:56,917 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-0
2020-07-12 20:53:56,918 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1917, 100, 1763, 100, 1989, 100, 100, 100, 100, 1890, 1775, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:53:56,919 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:53:56,919 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-2
2020-07-12 20:53:56,920 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1742, 100, 100, 1812, 100, 1778, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:53:56,921 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:53:56,921 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-3
2020-07-12 20:53:56,922 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 11085, 2050, 1989, 6921, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:53:56,923 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:53:56,923 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-4
2020-07-12 20:53:56,924 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1855, 100, 1916, 100, 100, 100, 100, 1812, 1865, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:53:56,925 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:53:56,925 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-0
2020-07-12 20:53:56,926 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1989, 100, 100, 100, 1989, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:54:11,311 genernal.py:  170             __init__() INFO     Saving features into cached file /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank [took 14.373 s]
2020-07-12 20:54:11,313 filelock.py:  315              release() DEBUG    Attempting to release lock 139670792534616 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:54:11,314 filelock.py:  318              release() INFO     Lock 139670792534616 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_128_smp-rank.lock
2020-07-12 20:54:11,317 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 20:54:12,357 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 20:54:14,210 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 20:54:18,179 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 20:54:20,557 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 20:54:23,947 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:54:24,918 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 20:54:24,987 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139670190328744 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_128_smp-rank.lock
2020-07-12 20:54:24,990 filelock.py:  274              acquire() INFO     Lock 139670190328744 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_128_smp-rank.lock
2020-07-12 20:54:24,992 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 20:54:25,650 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:54:25,651 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-1
2020-07-12 20:54:25,651 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 1742, 100, 100, 100, 1962, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:54:25,652 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:54:25,652 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-2
2020-07-12 20:54:25,653 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1989, 1018, 1011, 1014, 100, 100, 100, 100, 1989, 100, 100, 100, 100, 100, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:54:25,653 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:54:25,654 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-3
2020-07-12 20:54:25,654 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 2184, 1840, 1986, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:54:25,655 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:54:25,656 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-4
2020-07-12 20:54:25,656 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1855, 100, 100, 100, 100, 100, 100, 1916, 1815, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:54:25,657 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 20:54:25,657 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-5
2020-07-12 20:54:25,658 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1740, 100, 100, 100, 100, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 20:54:26,479 genernal.py:  170             __init__() INFO     Saving features into cached file /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_128_smp-rank [took 0.821 s]
2020-07-12 20:54:26,481 filelock.py:  315              release() DEBUG    Attempting to release lock 139670190328744 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_128_smp-rank.lock
2020-07-12 20:54:26,482 filelock.py:  318              release() INFO     Lock 139670190328744 released on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_128_smp-rank.lock
2020-07-12 20:54:30,150 trainer.py:  208             __init__() INFO     You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
2020-07-12 20:54:30,191 trainer.py:  440                train() INFO     ***** Running training *****
2020-07-12 20:54:30,192 trainer.py:  441                train() INFO       Num examples = 70206
2020-07-12 20:54:30,195 trainer.py:  442                train() INFO       Num Epochs = 3
2020-07-12 20:54:30,197 trainer.py:  443                train() INFO       Instantaneous batch size per device = 32
2020-07-12 20:54:30,198 trainer.py:  444                train() INFO       Total train batch size (w. parallel, distributed & accumulation) = 64
2020-07-12 20:54:30,199 trainer.py:  445                train() INFO       Gradient Accumulation steps = 1
2020-07-12 20:54:30,200 trainer.py:  446                train() INFO       Total optimization steps = 3291
2020-07-12 20:54:30,201 trainer.py:  468                train() INFO       Starting fine-tuning.
2020-07-12 20:57:29,282 trainer.py:  609                 _log() INFO     {'loss': 1.3557185957431792, 'learning_rate': 1.6961409905803706e-05, 'epoch': 0.45578851412944393, 'step': 500}
2020-07-12 20:57:29,287 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-500
2020-07-12 20:57:29,292 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-500/config.json
2020-07-12 20:57:30,089 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-500/pytorch_model.bin
2020-07-12 21:00:29,244 trainer.py:  609                 _log() INFO     {'loss': 1.2394890981912612, 'learning_rate': 1.3922819811607416e-05, 'epoch': 0.9115770282588879, 'step': 1000}
2020-07-12 21:00:29,251 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-1000
2020-07-12 21:00:29,257 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-1000/config.json
2020-07-12 21:00:30,050 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-1000/pytorch_model.bin
2020-07-12 21:03:29,051 trainer.py:  609                 _log() INFO     {'loss': 1.1953029365539551, 'learning_rate': 1.0884229717411122e-05, 'epoch': 1.3673655423883317, 'step': 1500}
2020-07-12 21:03:29,056 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-1500
2020-07-12 21:03:29,064 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-1500/config.json
2020-07-12 21:03:29,848 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-1500/pytorch_model.bin
2020-07-12 21:06:27,924 trainer.py:  609                 _log() INFO     {'loss': 1.1785382839441298, 'learning_rate': 7.84563962321483e-06, 'epoch': 1.8231540565177757, 'step': 2000}
2020-07-12 21:06:27,928 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-2000
2020-07-12 21:06:27,934 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-2000/config.json
2020-07-12 21:06:28,709 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-2000/pytorch_model.bin
2020-07-12 21:09:27,500 trainer.py:  609                 _log() INFO     {'loss': 1.1468070610761643, 'learning_rate': 4.807049529018536e-06, 'epoch': 2.2789425706472195, 'step': 2500}
2020-07-12 21:09:27,546 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-2500
2020-07-12 21:09:27,551 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-2500/config.json
2020-07-12 21:09:28,373 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-2500/pytorch_model.bin
2020-07-12 21:12:27,473 trainer.py:  609                 _log() INFO     {'loss': 1.1293129154443742, 'learning_rate': 1.7684594348222425e-06, 'epoch': 2.7347310847766635, 'step': 3000}
2020-07-12 21:12:27,481 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-3000
2020-07-12 21:12:27,488 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-3000/config.json
2020-07-12 21:12:28,286 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-3000/pytorch_model.bin
2020-07-12 21:14:12,804 trainer.py:  578                train() INFO     

Training completed. Do not forget to share your model on huggingface.co/models =)


2020-07-12 21:14:12,808 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank
2020-07-12 21:14:12,870 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/config.json
2020-07-12 21:14:13,762 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/pytorch_model.bin
2020-07-12 21:14:13,814 run_text_classification.py:  177                 main() INFO     *** Evaluate ***
2020-07-12 21:14:13,816 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 21:14:13,817 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 21:14:13,818 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 21:33:29,312 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 21:33:29,333 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False
2020-07-12 21:33:29,334 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 21:33:29,341 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 21:33:29,343 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 21:33:29,348 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 21:33:29,349 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 21:33:29,350 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 21:33:29,352 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 21:33:29,353 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 21:33:29,354 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 21:33:29,355 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 21:33:29,356 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 21:33:29,356 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 21:33:29,357 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 21:33:29,358 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 21:33:29,358 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 21:33:29,417 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 21:33:32,988 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 21:33:32,989 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 21:33:32,995 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 21:33:34,032 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 21:33:35,880 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 21:33:39,860 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 21:33:42,210 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 21:33:45,593 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 21:33:46,258 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 21:33:46,321 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140080029178848 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 21:33:46,324 filelock.py:  274              acquire() INFO     Lock 140080029178848 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 21:33:46,325 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 21:33:59,631 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 21:33:59,632 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-0
2020-07-12 21:33:59,633 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1917, 100, 1763, 100, 1989, 100, 100, 100, 100, 1890, 1775, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 21:33:59,634 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 21:33:59,634 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-2
2020-07-12 21:33:59,635 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1742, 100, 100, 1812, 100, 1778, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 21:33:59,636 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 21:33:59,637 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-3
2020-07-12 21:33:59,637 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 11085, 2050, 1989, 6921, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 21:33:59,638 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 21:33:59,638 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-4
2020-07-12 21:33:59,638 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1855, 100, 1916, 100, 100, 100, 100, 1812, 1865, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 21:33:59,639 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 21:33:59,639 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-0
2020-07-12 21:33:59,640 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1989, 100, 100, 100, 1989, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 21:34:06,227 genernal.py:  170             __init__() INFO     Saving features into cached file /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank [took 6.582 s]
2020-07-12 21:34:06,229 filelock.py:  315              release() DEBUG    Attempting to release lock 140080029178848 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 21:34:06,230 filelock.py:  318              release() INFO     Lock 140080029178848 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 21:34:06,234 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 21:34:07,292 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 21:34:09,149 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 21:34:14,050 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 21:34:16,367 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 21:34:19,856 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 21:34:20,692 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 21:34:20,759 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140079089847600 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 21:34:20,762 filelock.py:  274              acquire() INFO     Lock 140079089847600 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 21:34:20,764 genernal.py:  122             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-12 21:34:21,384 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 21:34:21,385 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-1
2020-07-12 21:34:21,385 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 1742, 100, 100, 100, 1962, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 21:34:21,386 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 21:34:21,387 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-2
2020-07-12 21:34:21,387 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1989, 1018, 1011, 1014, 100, 100, 100, 100, 1989, 100, 100, 100, 100, 100, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 21:34:21,388 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 21:34:21,388 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-3
2020-07-12 21:34:21,389 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 2184, 1840, 1986, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 21:34:21,389 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 21:34:21,389 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-4
2020-07-12 21:34:21,390 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1855, 100, 100, 100, 100, 100, 100, 1916, 1815, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 21:34:21,390 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-12 21:34:21,391 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: test-5
2020-07-12 21:34:21,392 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1740, 100, 100, 100, 100, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-12 21:34:22,337 genernal.py:  170             __init__() INFO     Saving features into cached file /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank [took 0.944 s]
2020-07-12 21:34:22,338 filelock.py:  315              release() DEBUG    Attempting to release lock 140079089847600 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 21:34:22,339 filelock.py:  318              release() INFO     Lock 140079089847600 released on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 21:34:25,917 trainer.py:  208             __init__() INFO     You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
2020-07-12 21:34:25,961 trainer.py:  440                train() INFO     ***** Running training *****
2020-07-12 21:34:25,962 trainer.py:  441                train() INFO       Num examples = 70206
2020-07-12 21:34:25,969 trainer.py:  442                train() INFO       Num Epochs = 1
2020-07-12 21:34:25,972 trainer.py:  443                train() INFO       Instantaneous batch size per device = 32
2020-07-12 21:34:25,973 trainer.py:  444                train() INFO       Total train batch size (w. parallel, distributed & accumulation) = 64
2020-07-12 21:34:25,974 trainer.py:  445                train() INFO       Gradient Accumulation steps = 1
2020-07-12 21:34:25,975 trainer.py:  446                train() INFO       Total optimization steps = 1097
2020-07-12 21:34:25,975 trainer.py:  468                train() INFO       Starting fine-tuning.
2020-07-12 21:36:21,092 trainer.py:  609                 _log() INFO     {'loss': 1.3512800500392914, 'learning_rate': 1.0884229717411122e-05, 'epoch': 0.45578851412944393, 'step': 500}
2020-07-12 21:36:21,095 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-500
2020-07-12 21:36:21,101 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-500/config.json
2020-07-12 21:36:21,995 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-500/pytorch_model.bin
2020-07-12 21:38:15,594 trainer.py:  609                 _log() INFO     {'loss': 1.2427901173830032, 'learning_rate': 1.7684594348222425e-06, 'epoch': 0.9115770282588879, 'step': 1000}
2020-07-12 21:38:15,596 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 21:38:15,597 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 21:38:15,598 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 21:38:28,261 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2311558986180706, 'eval_acc': 0.49040043883708173, 'epoch': 0.9115770282588879, 'step': 1000}
2020-07-12 21:38:28,264 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-1000
2020-07-12 21:38:28,270 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-1000/config.json
2020-07-12 21:38:29,076 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-1000/pytorch_model.bin
2020-07-12 21:38:52,693 trainer.py:  578                train() INFO     

Training completed. Do not forget to share your model on huggingface.co/models =)


2020-07-12 21:38:52,696 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank
2020-07-12 21:38:52,702 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/config.json
2020-07-12 21:38:53,556 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/pytorch_model.bin
2020-07-12 21:38:53,603 run_text_classification.py:  180                 main() INFO     *** Evaluate ***
2020-07-12 21:38:53,605 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 21:38:53,606 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 21:38:53,607 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 21:39:06,292 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2311653332752095, 'eval_acc': 0.4890290729566648, 'epoch': 1.0, 'step': 1097}
2020-07-12 21:39:06,340 run_text_classification.py:  199                 main() INFO     ***** Eval results smp-rank *****
2020-07-12 21:39:06,341 run_text_classification.py:  201                 main() INFO       eval_loss = 1.2311653332752095
2020-07-12 21:39:06,341 run_text_classification.py:  201                 main() INFO       eval_acc = 0.4890290729566648
2020-07-12 21:39:06,342 run_text_classification.py:  201                 main() INFO       epoch = 1.0
2020-07-12 21:43:43,143 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 21:43:43,163 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False
2020-07-12 21:43:43,165 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 21:43:43,171 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 21:43:43,174 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 21:43:43,179 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 21:43:43,180 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 21:43:43,181 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 21:43:43,183 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 21:43:43,184 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 21:43:43,185 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 21:43:43,186 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 21:43:43,187 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 21:43:43,188 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 21:43:43,188 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 21:43:43,189 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 21:43:43,190 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 21:43:43,245 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 21:43:46,915 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 21:43:46,916 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 21:43:46,923 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 21:43:47,960 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 21:43:49,764 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 21:43:53,708 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 21:43:56,045 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 21:43:59,456 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 21:44:00,089 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 21:44:00,151 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140663757204560 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 21:44:00,153 filelock.py:  274              acquire() INFO     Lock 140663757204560 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 21:44:01,595 genernal.py:  119             __init__() INFO     Loading features from cached file /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank [took 1.440 s]
2020-07-12 21:44:01,598 filelock.py:  315              release() DEBUG    Attempting to release lock 140663757204560 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 21:44:01,599 filelock.py:  318              release() INFO     Lock 140663757204560 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 21:44:01,603 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 21:44:02,638 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 21:44:04,453 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 21:44:08,492 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 21:44:11,205 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 21:44:14,642 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 21:44:15,019 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 21:44:15,093 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140661227059184 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 21:44:15,095 filelock.py:  274              acquire() INFO     Lock 140661227059184 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 21:44:15,148 genernal.py:  119             __init__() INFO     Loading features from cached file /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank [took 0.051 s]
2020-07-12 21:44:15,149 filelock.py:  315              release() DEBUG    Attempting to release lock 140661227059184 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 21:44:15,150 filelock.py:  318              release() INFO     Lock 140661227059184 released on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 21:44:19,179 trainer.py:  208             __init__() INFO     You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
2020-07-12 21:44:19,207 trainer.py:  440                train() INFO     ***** Running training *****
2020-07-12 21:44:19,208 trainer.py:  441                train() INFO       Num examples = 70206
2020-07-12 21:44:19,212 trainer.py:  442                train() INFO       Num Epochs = 3
2020-07-12 21:44:19,212 trainer.py:  443                train() INFO       Instantaneous batch size per device = 32
2020-07-12 21:44:19,213 trainer.py:  444                train() INFO       Total train batch size (w. parallel, distributed & accumulation) = 64
2020-07-12 21:44:19,213 trainer.py:  445                train() INFO       Gradient Accumulation steps = 1
2020-07-12 21:44:19,214 trainer.py:  446                train() INFO       Total optimization steps = 3291
2020-07-12 21:44:19,215 trainer.py:  468                train() INFO       Starting fine-tuning.
2020-07-12 21:46:20,604 trainer.py:  609                 _log() INFO     {'loss': 1.3485112614631654, 'learning_rate': 1.6961409905803706e-05, 'epoch': 0.45578851412944393, 'step': 500}
2020-07-12 21:48:17,319 trainer.py:  609                 _log() INFO     {'loss': 1.238294193983078, 'learning_rate': 1.3922819811607416e-05, 'epoch': 0.9115770282588879, 'step': 1000}
2020-07-12 21:48:17,321 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 21:48:17,322 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 21:48:17,322 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 21:48:29,963 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2264500694316731, 'eval_acc': 0.4983543609434997, 'epoch': 0.9115770282588879, 'step': 1000}
2020-07-12 21:50:26,264 trainer.py:  609                 _log() INFO     {'loss': 1.194937386751175, 'learning_rate': 1.0884229717411122e-05, 'epoch': 1.3673655423883317, 'step': 1500}
2020-07-12 21:52:22,509 trainer.py:  609                 _log() INFO     {'loss': 1.1742282419204713, 'learning_rate': 7.84563962321483e-06, 'epoch': 1.8231540565177757, 'step': 2000}
2020-07-12 21:52:22,510 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 21:52:22,512 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 21:52:22,512 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 21:52:35,364 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2035814724993288, 'eval_acc': 0.49917718047174986, 'epoch': 1.8231540565177757, 'step': 2000}
2020-07-12 21:54:31,040 trainer.py:  609                 _log() INFO     {'loss': 1.1464011495113373, 'learning_rate': 4.807049529018536e-06, 'epoch': 2.2789425706472195, 'step': 2500}
2020-07-12 21:56:27,135 trainer.py:  609                 _log() INFO     {'loss': 1.134060686826706, 'learning_rate': 1.7684594348222425e-06, 'epoch': 2.7347310847766635, 'step': 3000}
2020-07-12 21:56:27,137 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 21:56:27,138 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 21:56:27,139 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 21:56:40,056 trainer.py:  609                 _log() INFO     {'eval_loss': 1.188554387997117, 'eval_acc': 0.5106966538672518, 'epoch': 2.7347310847766635, 'step': 3000}
2020-07-12 21:57:47,845 trainer.py:  578                train() INFO     

Training completed. Do not forget to share your model on huggingface.co/models =)


2020-07-12 21:57:47,850 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank
2020-07-12 21:57:47,856 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/config.json
2020-07-12 21:57:48,628 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/pytorch_model.bin
2020-07-12 21:57:48,675 run_text_classification.py:  180                 main() INFO     *** Evaluate ***
2020-07-12 21:57:48,677 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 21:57:48,678 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 21:57:48,679 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 21:58:01,579 trainer.py:  609                 _log() INFO     {'eval_loss': 1.1878156778320932, 'eval_acc': 0.5076796489303346, 'epoch': 3.0, 'step': 3291}
2020-07-12 21:58:01,584 run_text_classification.py:  199                 main() INFO     ***** Eval results smp-rank *****
2020-07-12 21:58:01,585 run_text_classification.py:  201                 main() INFO       eval_loss = 1.1878156778320932
2020-07-12 21:58:01,587 run_text_classification.py:  201                 main() INFO       eval_acc = 0.5076796489303346
2020-07-12 21:58:01,587 run_text_classification.py:  201                 main() INFO       epoch = 3.0
2020-07-12 22:03:28,860 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 22:03:28,886 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False
2020-07-12 22:03:28,887 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 22:03:28,893 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 22:03:28,896 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 22:03:28,900 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 22:03:28,902 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 22:03:28,903 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 22:03:28,905 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 22:03:28,906 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 22:03:28,907 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 22:03:28,908 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 22:03:28,909 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 22:03:28,909 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 22:03:28,910 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 22:03:28,911 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 22:03:28,911 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 22:03:28,963 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 22:03:32,854 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 22:03:32,855 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 22:03:32,861 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 22:03:33,895 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 22:03:35,753 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 22:03:39,689 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 22:03:42,024 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 22:03:45,459 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 22:03:46,116 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 22:03:46,181 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139977785301984 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 22:03:46,183 filelock.py:  274              acquire() INFO     Lock 139977785301984 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 22:03:47,779 genernal.py:  119             __init__() INFO     Loading features from cached file /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank [took 1.594 s]
2020-07-12 22:03:47,782 filelock.py:  315              release() DEBUG    Attempting to release lock 139977785301984 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 22:03:47,783 filelock.py:  318              release() INFO     Lock 139977785301984 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 22:03:47,787 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 22:03:48,828 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 22:03:50,691 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 22:03:54,765 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 22:03:57,570 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 22:04:01,008 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 22:04:01,407 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 22:04:01,477 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139975706774832 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 22:04:01,479 filelock.py:  274              acquire() INFO     Lock 139975706774832 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 22:04:01,532 genernal.py:  119             __init__() INFO     Loading features from cached file /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank [took 0.051 s]
2020-07-12 22:04:01,533 filelock.py:  315              release() DEBUG    Attempting to release lock 139975706774832 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 22:04:01,534 filelock.py:  318              release() INFO     Lock 139975706774832 released on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 22:04:05,035 trainer.py:  208             __init__() INFO     You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
2020-07-12 22:04:05,066 trainer.py:  440                train() INFO     ***** Running training *****
2020-07-12 22:04:05,067 trainer.py:  441                train() INFO       Num examples = 70206
2020-07-12 22:04:05,071 trainer.py:  442                train() INFO       Num Epochs = 5
2020-07-12 22:04:05,072 trainer.py:  443                train() INFO       Instantaneous batch size per device = 32
2020-07-12 22:04:05,073 trainer.py:  444                train() INFO       Total train batch size (w. parallel, distributed & accumulation) = 64
2020-07-12 22:04:05,073 trainer.py:  445                train() INFO       Gradient Accumulation steps = 1
2020-07-12 22:04:05,074 trainer.py:  446                train() INFO       Total optimization steps = 5485
2020-07-12 22:04:05,075 trainer.py:  468                train() INFO       Starting fine-tuning.
2020-07-12 22:05:59,422 trainer.py:  609                 _log() INFO     {'loss': 1.332721562385559, 'learning_rate': 4.5442114858705564e-05, 'epoch': 0.45578851412944393, 'step': 500}
2020-07-12 22:07:50,767 trainer.py:  609                 _log() INFO     {'loss': 1.226229334115982, 'learning_rate': 4.088422971741112e-05, 'epoch': 0.9115770282588879, 'step': 1000}
2020-07-12 22:07:50,769 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:07:50,770 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:07:50,771 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:08:03,458 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2143901421835548, 'eval_acc': 0.5008228195282501, 'epoch': 0.9115770282588879, 'step': 1000}
2020-07-12 22:09:54,799 trainer.py:  609                 _log() INFO     {'loss': 1.1752317372560501, 'learning_rate': 3.632634457611668e-05, 'epoch': 1.3673655423883317, 'step': 1500}
2020-07-12 22:11:45,725 trainer.py:  609                 _log() INFO     {'loss': 1.1523578308820726, 'learning_rate': 3.176845943482224e-05, 'epoch': 1.8231540565177757, 'step': 2000}
2020-07-12 22:11:45,727 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:11:45,728 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:11:45,729 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:11:58,339 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2120155149645973, 'eval_acc': 0.5032912781130006, 'epoch': 1.8231540565177757, 'step': 2000}
2020-07-12 22:13:49,592 trainer.py:  609                 _log() INFO     {'loss': 1.1076713248491288, 'learning_rate': 2.72105742935278e-05, 'epoch': 2.2789425706472195, 'step': 2500}
2020-07-12 22:15:40,424 trainer.py:  609                 _log() INFO     {'loss': 1.0923444205522537, 'learning_rate': 2.2652689152233363e-05, 'epoch': 2.7347310847766635, 'step': 3000}
2020-07-12 22:15:40,426 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:15:40,428 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:15:40,428 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:15:53,063 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2061448243626378, 'eval_acc': 0.518102029621503, 'epoch': 2.7347310847766635, 'step': 3000}
2020-07-12 22:17:44,280 trainer.py:  609                 _log() INFO     {'loss': 1.059553080677986, 'learning_rate': 1.8094804010938925e-05, 'epoch': 3.1905195989061075, 'step': 3500}
2020-07-12 22:19:35,545 trainer.py:  609                 _log() INFO     {'loss': 1.0202027949094772, 'learning_rate': 1.3536918869644485e-05, 'epoch': 3.6463081130355515, 'step': 4000}
2020-07-12 22:19:35,546 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:19:35,547 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:19:35,548 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:19:48,654 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2202733970786397, 'eval_acc': 0.5211190345584202, 'epoch': 3.6463081130355515, 'step': 4000}
2020-07-12 22:21:39,959 trainer.py:  609                 _log() INFO     {'loss': 1.0011619012355804, 'learning_rate': 8.979033728350047e-06, 'epoch': 4.102096627164995, 'step': 4500}
2020-07-12 22:23:30,531 trainer.py:  609                 _log() INFO     {'loss': 0.9621970032453537, 'learning_rate': 4.421148587055606e-06, 'epoch': 4.557885141294439, 'step': 5000}
2020-07-12 22:23:30,533 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:23:30,534 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:23:30,534 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:23:43,474 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2378497319786173, 'eval_acc': 0.521667580910587, 'epoch': 4.557885141294439, 'step': 5000}
2020-07-12 22:23:43,540 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-5000
2020-07-12 22:23:43,547 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-5000/config.json
2020-07-12 22:23:44,310 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-5000/pytorch_model.bin
2020-07-12 22:25:33,920 trainer.py:  578                train() INFO     

Training completed. Do not forget to share your model on huggingface.co/models =)


2020-07-12 22:25:33,924 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank
2020-07-12 22:25:33,931 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/config.json
2020-07-12 22:25:34,752 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/pytorch_model.bin
2020-07-12 22:25:34,796 run_text_classification.py:  180                 main() INFO     *** Evaluate ***
2020-07-12 22:25:34,798 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:25:34,799 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:25:34,800 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:25:47,407 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2313366358478863, 'eval_acc': 0.5241360394953374, 'epoch': 5.0, 'step': 5485}
2020-07-12 22:25:47,411 run_text_classification.py:  199                 main() INFO     ***** Eval results smp-rank *****
2020-07-12 22:25:47,412 run_text_classification.py:  201                 main() INFO       eval_loss = 1.2313366358478863
2020-07-12 22:25:47,413 run_text_classification.py:  201                 main() INFO       eval_acc = 0.5241360394953374
2020-07-12 22:25:47,413 run_text_classification.py:  201                 main() INFO       epoch = 5.0
2020-07-12 22:33:18,307 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-12 22:33:18,332 run_text_classification.py:   89                 main() WARNING  Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False
2020-07-12 22:33:18,336 run_text_classification.py:   91                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=4e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-12 22:33:18,343 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 22:33:18,345 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 22:33:18,350 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-12 22:33:18,351 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-12 22:33:18,353 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-12 22:33:18,354 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-12 22:33:18,356 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-12 22:33:18,357 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-12 22:33:18,358 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-12 22:33:18,359 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-12 22:33:18,360 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 22:33:18,360 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 22:33:18,361 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 22:33:18,361 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-12 22:33:18,415 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-12 22:33:21,645 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-12 22:33:21,646 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-12 22:33:21,652 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 22:33:22,698 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 22:33:24,504 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 22:33:28,546 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 22:33:30,898 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 22:33:34,402 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 22:33:34,971 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 22:33:35,021 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139659956485312 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 22:33:35,022 filelock.py:  274              acquire() INFO     Lock 139659956485312 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 22:33:36,343 genernal.py:  116             __init__() INFO     Loading features from cached file /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank [took 1.319 s]
2020-07-12 22:33:36,345 filelock.py:  315              release() DEBUG    Attempting to release lock 139659956485312 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 22:33:36,347 filelock.py:  318              release() INFO     Lock 139659956485312 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-12 22:33:36,350 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-12 22:33:37,589 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-12 22:33:39,424 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-12 22:33:43,464 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-12 22:33:46,018 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-12 22:33:49,532 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 22:33:49,893 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-12 22:33:49,960 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 139658051318056 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 22:33:49,962 filelock.py:  274              acquire() INFO     Lock 139658051318056 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 22:33:50,012 genernal.py:  116             __init__() INFO     Loading features from cached file /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank [took 0.049 s]
2020-07-12 22:33:50,013 filelock.py:  315              release() DEBUG    Attempting to release lock 139658051318056 on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 22:33:50,015 filelock.py:  318              release() INFO     Lock 139658051318056 released on /dfsdata2/yucc1_data/projects/smp/data/cached_dev_BertTokenizer_50_smp-rank.lock
2020-07-12 22:33:53,429 trainer.py:  208             __init__() INFO     You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
2020-07-12 22:33:53,456 trainer.py:  440                train() INFO     ***** Running training *****
2020-07-12 22:33:53,457 trainer.py:  441                train() INFO       Num examples = 70206
2020-07-12 22:33:53,460 trainer.py:  442                train() INFO       Num Epochs = 10
2020-07-12 22:33:53,460 trainer.py:  443                train() INFO       Instantaneous batch size per device = 32
2020-07-12 22:33:53,461 trainer.py:  444                train() INFO       Total train batch size (w. parallel, distributed & accumulation) = 64
2020-07-12 22:33:53,462 trainer.py:  445                train() INFO       Gradient Accumulation steps = 1
2020-07-12 22:33:53,463 trainer.py:  446                train() INFO       Total optimization steps = 10970
2020-07-12 22:33:53,464 trainer.py:  468                train() INFO       Starting fine-tuning.
2020-07-12 22:35:49,734 trainer.py:  609                 _log() INFO     {'loss': 1.325100096464157, 'learning_rate': 3.817684594348223e-05, 'epoch': 0.45578851412944393, 'step': 500}
2020-07-12 22:37:41,438 trainer.py:  609                 _log() INFO     {'loss': 1.2259193093776704, 'learning_rate': 3.635369188696445e-05, 'epoch': 0.9115770282588879, 'step': 1000}
2020-07-12 22:37:41,440 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:37:41,441 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:37:41,442 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:37:54,002 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2106452891439723, 'eval_acc': 0.5054854635216676, 'epoch': 0.9115770282588879, 'step': 1000}
2020-07-12 22:39:46,225 trainer.py:  609                 _log() INFO     {'loss': 1.1756267125606537, 'learning_rate': 3.453053783044668e-05, 'epoch': 1.3673655423883317, 'step': 1500}
2020-07-12 22:41:37,942 trainer.py:  609                 _log() INFO     {'loss': 1.1570538399219512, 'learning_rate': 3.27073837739289e-05, 'epoch': 1.8231540565177757, 'step': 2000}
2020-07-12 22:41:37,944 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:41:37,945 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:41:37,946 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:41:50,870 trainer.py:  609                 _log() INFO     {'eval_loss': 1.205681155256012, 'eval_acc': 0.5041140976412507, 'epoch': 1.8231540565177757, 'step': 2000}
2020-07-12 22:43:41,965 trainer.py:  609                 _log() INFO     {'loss': 1.1154562091827394, 'learning_rate': 3.088422971741112e-05, 'epoch': 2.2789425706472195, 'step': 2500}
2020-07-12 22:45:32,971 trainer.py:  609                 _log() INFO     {'loss': 1.1020315091609956, 'learning_rate': 2.9061075660893348e-05, 'epoch': 2.7347310847766635, 'step': 3000}
2020-07-12 22:45:32,973 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:45:32,974 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:45:32,975 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:45:45,938 trainer.py:  609                 _log() INFO     {'eval_loss': 1.203913485271889, 'eval_acc': 0.5164563905650027, 'epoch': 2.7347310847766635, 'step': 3000}
2020-07-12 22:47:37,271 trainer.py:  609                 _log() INFO     {'loss': 1.0735394332408905, 'learning_rate': 2.723792160437557e-05, 'epoch': 3.1905195989061075, 'step': 3500}
2020-07-12 22:49:28,607 trainer.py:  609                 _log() INFO     {'loss': 1.037513226747513, 'learning_rate': 2.5414767547857796e-05, 'epoch': 3.6463081130355515, 'step': 4000}
2020-07-12 22:49:28,609 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:49:28,610 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:49:28,610 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:49:41,100 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2472616175287647, 'eval_acc': 0.5027427317608338, 'epoch': 3.6463081130355515, 'step': 4000}
2020-07-12 22:51:32,898 trainer.py:  609                 _log() INFO     {'loss': 1.0201082985401153, 'learning_rate': 2.3591613491340023e-05, 'epoch': 4.102096627164995, 'step': 4500}
2020-07-12 22:53:24,099 trainer.py:  609                 _log() INFO     {'loss': 0.9749830086231231, 'learning_rate': 2.1768459434822244e-05, 'epoch': 4.557885141294439, 'step': 5000}
2020-07-12 22:53:24,101 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:53:24,103 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:53:24,104 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:53:36,679 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2644302800559162, 'eval_acc': 0.523861766319254, 'epoch': 4.557885141294439, 'step': 5000}
2020-07-12 22:53:36,682 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-5000
2020-07-12 22:53:36,738 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-5000/config.json
2020-07-12 22:53:37,581 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-5000/pytorch_model.bin
2020-07-12 22:55:30,648 trainer.py:  609                 _log() INFO     {'loss': 0.9803678662776947, 'learning_rate': 1.9945305378304468e-05, 'epoch': 5.013673655423883, 'step': 5500}
2020-07-12 22:57:23,137 trainer.py:  609                 _log() INFO     {'loss': 0.9220535249710083, 'learning_rate': 1.8122151321786692e-05, 'epoch': 5.469462169553327, 'step': 6000}
2020-07-12 22:57:23,139 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 22:57:23,139 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 22:57:23,140 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 22:57:35,683 trainer.py:  609                 _log() INFO     {'eval_loss': 1.2844635756653653, 'eval_acc': 0.511519473395502, 'epoch': 5.469462169553327, 'step': 6000}
2020-07-12 22:59:26,962 trainer.py:  609                 _log() INFO     {'loss': 0.9221263712644577, 'learning_rate': 1.6298997265268916e-05, 'epoch': 5.925250683682771, 'step': 6500}
2020-07-12 23:01:18,027 trainer.py:  609                 _log() INFO     {'loss': 0.8799093056917191, 'learning_rate': 1.4475843208751142e-05, 'epoch': 6.381039197812215, 'step': 7000}
2020-07-12 23:01:18,029 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 23:01:18,030 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 23:01:18,030 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 23:01:30,573 trainer.py:  609                 _log() INFO     {'eval_loss': 1.3492358743883015, 'eval_acc': 0.51947339550192, 'epoch': 6.381039197812215, 'step': 7000}
2020-07-12 23:03:22,014 trainer.py:  609                 _log() INFO     {'loss': 0.8651947042942048, 'learning_rate': 1.2652689152233364e-05, 'epoch': 6.836827711941659, 'step': 7500}
2020-07-12 23:05:13,273 trainer.py:  609                 _log() INFO     {'loss': 0.843808547616005, 'learning_rate': 1.0829535095715588e-05, 'epoch': 7.292616226071103, 'step': 8000}
2020-07-12 23:05:13,274 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 23:05:13,275 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 23:05:13,276 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 23:05:26,171 trainer.py:  609                 _log() INFO     {'eval_loss': 1.3938473395088262, 'eval_acc': 0.5167306637410861, 'epoch': 7.292616226071103, 'step': 8000}
2020-07-12 23:07:17,589 trainer.py:  609                 _log() INFO     {'loss': 0.8266197974681855, 'learning_rate': 9.006381039197814e-06, 'epoch': 7.748404740200547, 'step': 8500}
2020-07-12 23:09:09,482 trainer.py:  609                 _log() INFO     {'loss': 0.8042448102831841, 'learning_rate': 7.183226982680037e-06, 'epoch': 8.20419325432999, 'step': 9000}
2020-07-12 23:09:09,484 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 23:09:09,485 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 23:09:09,486 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 23:09:22,474 trainer.py:  609                 _log() INFO     {'eval_loss': 1.410735099200617, 'eval_acc': 0.5235874931431705, 'epoch': 8.20419325432999, 'step': 9000}
2020-07-12 23:11:14,114 trainer.py:  609                 _log() INFO     {'loss': 0.7779171968698502, 'learning_rate': 5.360072926162261e-06, 'epoch': 8.659981768459435, 'step': 9500}
2020-07-12 23:13:06,624 trainer.py:  609                 _log() INFO     {'loss': 0.7795622162222863, 'learning_rate': 3.536918869644485e-06, 'epoch': 9.115770282588878, 'step': 10000}
2020-07-12 23:13:06,626 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 23:13:06,627 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 23:13:06,627 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 23:13:19,194 trainer.py:  609                 _log() INFO     {'eval_loss': 1.4278317421936153, 'eval_acc': 0.515907844212836, 'epoch': 9.115770282588878, 'step': 10000}
2020-07-12 23:13:19,268 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank/checkpoint-10000
2020-07-12 23:13:19,334 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-10000/config.json
2020-07-12 23:13:20,116 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/checkpoint-10000/pytorch_model.bin
2020-07-12 23:15:13,336 trainer.py:  609                 _log() INFO     {'loss': 0.7486118541955948, 'learning_rate': 1.7137648131267093e-06, 'epoch': 9.571558796718323, 'step': 10500}
2020-07-12 23:16:58,188 trainer.py:  578                train() INFO     

Training completed. Do not forget to share your model on huggingface.co/models =)


2020-07-12 23:16:58,192 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank
2020-07-12 23:16:58,199 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank/config.json
2020-07-12 23:16:59,020 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank/pytorch_model.bin
2020-07-12 23:16:59,064 run_text_classification.py:  180                 main() INFO     *** Evaluate ***
2020-07-12 23:16:59,066 trainer.py:  799     _prediction_loop() INFO     ***** Running Evaluation *****
2020-07-12 23:16:59,067 trainer.py:  800     _prediction_loop() INFO       Num examples = 3646
2020-07-12 23:16:59,068 trainer.py:  801     _prediction_loop() INFO       Batch size = 16
2020-07-12 23:17:11,631 trainer.py:  609                 _log() INFO     {'eval_loss': 1.4575029327942615, 'eval_acc': 0.5167306637410861, 'epoch': 10.0, 'step': 10970}
2020-07-12 23:17:11,636 run_text_classification.py:  199                 main() INFO     ***** Eval results smp-rank *****
2020-07-12 23:17:11,637 run_text_classification.py:  201                 main() INFO       eval_loss = 1.4575029327942615
2020-07-12 23:17:11,638 run_text_classification.py:  201                 main() INFO       eval_acc = 0.5167306637410861
2020-07-12 23:17:11,639 run_text_classification.py:  201                 main() INFO       epoch = 10.0
2020-07-13 07:32:52,385 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-13 07:32:52,416 run_text_classification.py:   71                 main() WARNING  Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False
2020-07-13 07:32:52,417 run_text_classification.py:   73                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank-online', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=4e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-13 07:32:52,425 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-13 07:32:52,427 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-13 07:32:52,432 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-13 07:32:52,434 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-13 07:32:52,435 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-13 07:32:52,437 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-13 07:32:52,438 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-13 07:32:52,439 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-13 07:32:52,440 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-13 07:32:52,441 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-13 07:32:52,442 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:32:52,443 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:32:52,443 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:32:52,444 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:32:52,494 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-13 07:32:55,572 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-13 07:32:55,574 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-13 07:32:55,580 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-13 07:32:56,648 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-13 07:32:58,591 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-13 07:33:02,713 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-13 07:33:05,152 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-13 07:33:08,772 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-13 07:33:09,305 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-13 07:33:09,351 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140379656887152 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:33:09,354 filelock.py:  274              acquire() INFO     Lock 140379656887152 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:33:10,490 genernal.py:  116             __init__() INFO     Loading features from cached file /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank [took 1.135 s]
2020-07-13 07:33:10,492 filelock.py:  315              release() DEBUG    Attempting to release lock 140379656887152 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:33:10,494 filelock.py:  318              release() INFO     Lock 140379656887152 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:33:14,316 trainer.py:  208             __init__() INFO     You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
2020-07-13 07:33:14,350 trainer.py:  440                train() INFO     ***** Running training *****
2020-07-13 07:33:14,352 trainer.py:  441                train() INFO       Num examples = 70206
2020-07-13 07:33:14,356 trainer.py:  442                train() INFO       Num Epochs = 5
2020-07-13 07:33:14,357 trainer.py:  443                train() INFO       Instantaneous batch size per device = 32
2020-07-13 07:33:14,358 trainer.py:  444                train() INFO       Total train batch size (w. parallel, distributed & accumulation) = 64
2020-07-13 07:33:14,359 trainer.py:  445                train() INFO       Gradient Accumulation steps = 1
2020-07-13 07:33:14,359 trainer.py:  446                train() INFO       Total optimization steps = 5485
2020-07-13 07:33:14,360 trainer.py:  468                train() INFO       Starting fine-tuning.
2020-07-13 07:35:08,927 trainer.py:  609                 _log() INFO     {'loss': 1.326168319940567, 'learning_rate': 3.635369188696445e-05, 'epoch': 0.45578851412944393, 'step': 500}
2020-07-13 07:36:59,719 trainer.py:  609                 _log() INFO     {'loss': 1.2236137087345123, 'learning_rate': 3.27073837739289e-05, 'epoch': 0.9115770282588879, 'step': 1000}
2020-07-13 07:38:50,704 trainer.py:  609                 _log() INFO     {'loss': 1.1735802229642869, 'learning_rate': 2.9061075660893348e-05, 'epoch': 1.3673655423883317, 'step': 1500}
2020-07-13 07:40:41,597 trainer.py:  609                 _log() INFO     {'loss': 1.1594387128353119, 'learning_rate': 2.5414767547857796e-05, 'epoch': 1.8231540565177757, 'step': 2000}
2020-07-13 07:44:36,259 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-13 07:44:36,286 run_text_classification.py:   74                 main() WARNING  Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False
2020-07-13 07:44:36,287 run_text_classification.py:   76                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank-online', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=4e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-13 07:44:36,290 run_text_classification.py:   77                 main() INFO     Model arguments: ModelArguments(model_name_or_path='/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased', config_name=None, tokenizer_name=None, cache_dir=None)
2020-07-13 07:44:36,291 run_text_classification.py:   78                 main() INFO     Data Training arguments: GenernalDataTrainingArguments(task_name='smp-rank', data_dir='/dfsdata2/yucc1_data/projects/smp/data', max_seq_length=50, overwrite_cache=False, block_dir='/dfsdata2/yucc1_data/projects/smp/smp2020/blocklist', online=True)
2020-07-13 07:44:36,297 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-13 07:44:36,299 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-13 07:44:36,303 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-13 07:44:36,305 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-13 07:44:36,306 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-13 07:44:36,308 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-13 07:44:36,309 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-13 07:44:36,310 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-13 07:44:36,310 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-13 07:44:36,312 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-13 07:44:36,312 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:44:36,313 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:44:36,313 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:44:36,314 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:44:36,363 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-13 07:44:39,393 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-13 07:44:39,395 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-13 07:44:39,399 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-13 07:44:40,503 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-13 07:44:42,442 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-13 07:44:46,655 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-13 07:44:49,132 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-13 07:44:52,701 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-13 07:44:53,230 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-13 07:44:53,276 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140560531839688 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:44:53,279 filelock.py:  274              acquire() INFO     Lock 140560531839688 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:44:54,524 genernal.py:  116             __init__() INFO     Loading features from cached file /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank [took 1.244 s]
2020-07-13 07:44:54,526 filelock.py:  315              release() DEBUG    Attempting to release lock 140560531839688 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:44:54,527 filelock.py:  318              release() INFO     Lock 140560531839688 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:44:58,366 trainer.py:  208             __init__() INFO     You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
2020-07-13 07:44:58,396 trainer.py:  440                train() INFO     ***** Running training *****
2020-07-13 07:44:58,397 trainer.py:  441                train() INFO       Num examples = 70206
2020-07-13 07:44:58,400 trainer.py:  442                train() INFO       Num Epochs = 3
2020-07-13 07:44:58,401 trainer.py:  443                train() INFO       Instantaneous batch size per device = 32
2020-07-13 07:44:58,402 trainer.py:  444                train() INFO       Total train batch size (w. parallel, distributed & accumulation) = 64
2020-07-13 07:44:58,403 trainer.py:  445                train() INFO       Gradient Accumulation steps = 1
2020-07-13 07:44:58,404 trainer.py:  446                train() INFO       Total optimization steps = 3291
2020-07-13 07:44:58,404 trainer.py:  468                train() INFO       Starting fine-tuning.
2020-07-13 07:46:52,854 trainer.py:  609                 _log() INFO     {'loss': 1.3247957122325897, 'learning_rate': 3.392281981160741e-05, 'epoch': 0.45578851412944393, 'step': 500}
2020-07-13 07:48:00,956 training_args.py:  252       _setup_devices() INFO     PyTorch: setting up devices
2020-07-13 07:48:00,986 run_text_classification.py:   74                 main() WARNING  Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False
2020-07-13 07:48:00,987 run_text_classification.py:   76                 main() INFO     Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/smp-rank-online', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=4e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_dir='/dfsdata2/yucc1_data/runs', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)
2020-07-13 07:48:00,991 run_text_classification.py:   77                 main() INFO     Model arguments: ModelArguments(model_name_or_path='/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased', config_name=None, tokenizer_name=None, cache_dir=None)
2020-07-13 07:48:00,992 run_text_classification.py:   78                 main() INFO     Data Training arguments: GenernalDataTrainingArguments(task_name='smp-rank', data_dir='/dfsdata2/yucc1_data/projects/smp/data', max_seq_length=50, overwrite_cache=True, block_dir='/dfsdata2/yucc1_data/projects/smp/smp2020/blocklist', online=True)
2020-07-13 07:48:00,996 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-13 07:48:00,999 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "smp-rank",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-13 07:48:01,004 configuration_utils.py:  262      get_config_dict() INFO     loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json
2020-07-13 07:48:01,006 configuration_utils.py:  300            from_dict() INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-07-13 07:48:01,008 tokenization_utils_base.py: 1167     _from_pretrained() INFO     Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-07-13 07:48:01,009 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.
2020-07-13 07:48:01,011 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.
2020-07-13 07:48:01,012 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.
2020-07-13 07:48:01,013 tokenization_utils_base.py: 1197     _from_pretrained() INFO     Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.
2020-07-13 07:48:01,014 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt
2020-07-13 07:48:01,015 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:48:01,016 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:48:01,016 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:48:01,017 tokenization_utils_base.py: 1252     _from_pretrained() INFO     loading file None
2020-07-13 07:48:01,069 modeling_utils.py:  665      from_pretrained() INFO     loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin
2020-07-13 07:48:04,169 modeling_utils.py:  757      from_pretrained() WARNING  Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2020-07-13 07:48:04,170 modeling_utils.py:  768      from_pretrained() WARNING  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2020-07-13 07:48:04,177 genernal.py:  180             __init__() INFO     topic: 体育
2020-07-13 07:48:05,269 genernal.py:  180             __init__() INFO     topic: 数码产品
2020-07-13 07:48:07,120 genernal.py:  180             __init__() INFO     topic: 美食
2020-07-13 07:48:11,284 genernal.py:  180             __init__() INFO     topic: 音乐
2020-07-13 07:48:13,688 genernal.py:  180             __init__() INFO     topic: 电影
2020-07-13 07:48:17,275 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-13 07:48:17,816 genernal.py:  136 _smp_rank_data_process() INFO     block dir: /dfsdata2/yucc1_data/projects/smp/smp2020/blocklist
2020-07-13 07:48:17,863 filelock.py:  270              acquire() DEBUG    Attempting to acquire lock 140513056803528 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:48:17,865 filelock.py:  274              acquire() INFO     Lock 140513056803528 acquired on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:48:17,867 genernal.py:  119             __init__() INFO     Creating features from dataset file at /dfsdata2/yucc1_data/projects/smp/data
2020-07-13 07:48:30,744 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-13 07:48:30,746 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-0
2020-07-13 07:48:30,747 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1917, 100, 1763, 100, 1989, 100, 100, 100, 100, 1890, 1775, 100, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-13 07:48:30,747 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-13 07:48:30,748 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-2
2020-07-13 07:48:30,749 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1742, 100, 100, 1812, 100, 1778, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-13 07:48:30,750 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-13 07:48:30,750 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-3
2020-07-13 07:48:30,751 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 11085, 2050, 1989, 6921, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-13 07:48:30,752 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-13 07:48:30,752 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-4
2020-07-13 07:48:30,753 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 1855, 100, 1916, 100, 100, 100, 100, 1812, 1865, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-13 07:48:30,754 genernal.py:   56 genernal_convert_examples_to_features() INFO     *** Example ***
2020-07-13 07:48:30,755 genernal.py:   57 genernal_convert_examples_to_features() INFO     guid: train-0
2020-07-13 07:48:30,755 genernal.py:   58 genernal_convert_examples_to_features() INFO     features: InputFeatures(input_ids=[101, 100, 100, 1989, 100, 100, 100, 1989, 100, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)
2020-07-13 07:48:37,292 genernal.py:  167             __init__() INFO     Saving features into cached file /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank [took 6.532 s]
2020-07-13 07:48:37,294 filelock.py:  315              release() DEBUG    Attempting to release lock 140513056803528 on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:48:37,295 filelock.py:  318              release() INFO     Lock 140513056803528 released on /dfsdata2/yucc1_data/projects/smp/data/cached_train_BertTokenizer_50_smp-rank.lock
2020-07-13 07:48:41,131 trainer.py:  208             __init__() INFO     You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
2020-07-13 07:48:41,161 trainer.py:  440                train() INFO     ***** Running training *****
2020-07-13 07:48:41,162 trainer.py:  441                train() INFO       Num examples = 73852
2020-07-13 07:48:41,166 trainer.py:  442                train() INFO       Num Epochs = 5
2020-07-13 07:48:41,168 trainer.py:  443                train() INFO       Instantaneous batch size per device = 32
2020-07-13 07:48:41,169 trainer.py:  444                train() INFO       Total train batch size (w. parallel, distributed & accumulation) = 64
2020-07-13 07:48:41,169 trainer.py:  445                train() INFO       Gradient Accumulation steps = 1
2020-07-13 07:48:41,170 trainer.py:  446                train() INFO       Total optimization steps = 5770
2020-07-13 07:48:41,171 trainer.py:  468                train() INFO       Starting fine-tuning.
2020-07-13 07:50:36,965 trainer.py:  609                 _log() INFO     {'loss': 1.3338197185993195, 'learning_rate': 3.653379549393414e-05, 'epoch': 0.43327556325823224, 'step': 500}
2020-07-13 07:52:28,133 trainer.py:  609                 _log() INFO     {'loss': 1.2326361763477325, 'learning_rate': 3.306759098786829e-05, 'epoch': 0.8665511265164645, 'step': 1000}
2020-07-13 07:54:18,966 trainer.py:  609                 _log() INFO     {'loss': 1.1781973472833633, 'learning_rate': 2.9601386481802426e-05, 'epoch': 1.2998266897746968, 'step': 1500}
2020-07-13 07:56:10,277 trainer.py:  609                 _log() INFO     {'loss': 1.1603623256683349, 'learning_rate': 2.6135181975736573e-05, 'epoch': 1.733102253032929, 'step': 2000}
2020-07-13 07:58:01,194 trainer.py:  609                 _log() INFO     {'loss': 1.1229980150461196, 'learning_rate': 2.2668977469670712e-05, 'epoch': 2.166377816291161, 'step': 2500}
2020-07-13 07:59:53,178 trainer.py:  609                 _log() INFO     {'loss': 1.0922048630714416, 'learning_rate': 1.9202772963604856e-05, 'epoch': 2.5996533795493937, 'step': 3000}
2020-07-13 08:01:44,032 trainer.py:  609                 _log() INFO     {'loss': 1.0809874929189682, 'learning_rate': 1.5736568457538996e-05, 'epoch': 3.032928942807626, 'step': 3500}
2020-07-13 08:03:35,624 trainer.py:  609                 _log() INFO     {'loss': 1.0291084316968917, 'learning_rate': 1.2270363951473139e-05, 'epoch': 3.466204506065858, 'step': 4000}
2020-07-13 08:05:26,591 trainer.py:  609                 _log() INFO     {'loss': 1.0256435968875885, 'learning_rate': 8.80415944540728e-06, 'epoch': 3.89948006932409, 'step': 4500}
2020-07-13 08:07:17,741 trainer.py:  609                 _log() INFO     {'loss': 0.9887541109323501, 'learning_rate': 5.337954939341422e-06, 'epoch': 4.332755632582322, 'step': 5000}
2020-07-13 08:07:17,746 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank-online/checkpoint-5000
2020-07-13 08:07:17,754 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank-online/checkpoint-5000/config.json
2020-07-13 08:07:18,493 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank-online/checkpoint-5000/pytorch_model.bin
2020-07-13 08:09:11,301 trainer.py:  609                 _log() INFO     {'loss': 0.9760181261301041, 'learning_rate': 1.8717504332755634e-06, 'epoch': 4.766031195840554, 'step': 5500}
2020-07-13 08:10:11,549 trainer.py:  578                train() INFO     

Training completed. Do not forget to share your model on huggingface.co/models =)


2020-07-13 08:10:11,555 trainer.py:  688                _save() INFO     Saving model checkpoint to /dfsdata2/yucc1_data/output/smp-rank-online
2020-07-13 08:10:11,564 configuration_utils.py:  142      save_pretrained() INFO     Configuration saved in /dfsdata2/yucc1_data/output/smp-rank-online/config.json
2020-07-13 08:10:12,299 modeling_utils.py:  507      save_pretrained() INFO     Model weights saved in /dfsdata2/yucc1_data/output/smp-rank-online/pytorch_model.bin
